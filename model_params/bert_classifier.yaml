epochs: 10
bert_model_name: distilbert-base-uncased
max_sequence_length: 64
learning_rate: 0.0001
batch_size: 32
classifier_init_state_path: model_params/init_classifier_params.pt
classifier_type: single-fc
classifier_hidden_size: 0
classifier_drop_out: 0
optimizer_weight_decay: 0.001
num_training_examples: 10000